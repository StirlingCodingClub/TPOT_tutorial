{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TPOT to fit optimized machine learning models in Python\n",
    "\n",
    "In this tutorial session we will explore how to use the packahe [TPOT](https://epistasislab.github.io/tpot/) to automatically choose the best machine learning algorithm and the optimal parameterization for the algorithm. We will show examples for classification and regression (in the machine learning sense, not the classic stats sense)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets dig right into our first example. First, we will import the necessasy packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a first example using a built in dataset called `digits`. You can learn more about it at https://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(digits.data,digits.target, train_size = 0.7, test_size = 0.3, random_state = 79)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first do a regular ML run, using Random Forests, to see which accuracies we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=79, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mod = RandomForestClassifier(n_estimators=100, random_state = 79)\n",
    "\n",
    "rf_mod.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.978"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred = rf_mod.predict(X_test)\n",
    "\n",
    "round(metrics.accuracy_score(Y_test,rf_pred),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good. What would TPOT give us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d521463e2b464eb79b8ffb5d86d4d349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=220, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.9562909048914667\n",
      "Generation 2 - Current best internal CV score: 0.9658404886184762\n",
      "Generation 3 - Current best internal CV score: 0.9666604190673738\n",
      "Generation 4 - Current best internal CV score: 0.9666604190673738\n",
      "Generation 5 - Current best internal CV score: 0.9674410934291178\n",
      "Generation 6 - Current best internal CV score: 0.9769783543070867\n",
      "Generation 7 - Current best internal CV score: 0.9769783543070867\n",
      "Generation 8 - Current best internal CV score: 0.9769783543070867\n"
     ]
    }
   ],
   "source": [
    "tpot_mod = TPOTClassifier(generations = 5, population_size = 100, verbosity = 2, n_jobs = 11, random_state = 79)\n",
    "\n",
    "tpot_mod.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.985"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_pred = tpot_mod.predict(X_test)\n",
    "\n",
    "round(metrics.accuracy_score(Y_test,tp_pred),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an accuracy of ~0.99 with only ten generations. If we let it run for longer, it could be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do we do withit then? Well, TPOT is nice enough to output it's \"findings\" as a ready to run Python script: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_mod.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of TPOT suggest that you take this as a suggested pipeline, and do further fine parameter tuning with grid search to eek out the last bits of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would TPOT do with a more sparse and complex dataset? Let's look at some data from Dhemerson Conciani a masters student working who is developing ML algorithms to classify burn scars in the State of São Paulo, Brazil. We have originally almost one million labelled pixels, but for this exercise we will use a subset of 1000 pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the data using `pandas`, the Python implementation of `data.frames`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set project folder as working folder\n",
    "os.chdir('/home/thiago/Projects/TPOT_tutorial/')\n",
    "\n",
    "# Read in csv data using pandas, letting it know there are no variable names\n",
    "burn = pd.read_csv('./data/burn_scar_mapping/burn_scars_1000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is in, it is always a good idea to inspect the data to make sure it looks alright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start pre-processing the data for ML as usual. Unlike R, Python ML algorithms want headerless numeric arrays for all predictor variables, and a separate numeric array for the dependent variable. It will not recognize \"factor\" variables like R does. So we need to encode the categorical variables as numeric, including the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we select only the columns we need for the model, and find out who the categorical columns are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we dont want\n",
    "vars_df = burn.drop(columns = ['Scene','Date'])\n",
    "\n",
    "# Make categorical variable boolean mask\n",
    "categorical_feature_mask = vars_df.dtypes == object # filter categorical columns using mask and turn it into a list\n",
    "print(categorical_feature_mask)\n",
    "\n",
    "# Select columns\n",
    "categorical_cols = vars_df.columns[categorical_feature_mask].tolist()\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the encoding function of `scikit-learn`. You can think of this as a simple model that transforms labels into digits, so the process is similar to actually fitting a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# instantiate the Encoder, as you would with a Classifier or a Regressor\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Empty dataframe to save labels\n",
    "labels = pd.DataFrame()\n",
    "\n",
    "# Apply the encoder on categorical feature columns\n",
    "for col in categorical_cols:\n",
    "        vars_df[col] = le.fit_transform(vars_df[col])\n",
    "        cl = list(le.classes_)\n",
    "        labels = pd.concat([labels, pd.DataFrame({col: cl})],sort = False)\n",
    "\n",
    "print(labels)                           \n",
    "vars_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to separate the X and Y variables as individual arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vars_df.iloc[:,1:].values\n",
    "Y = vars_df.iloc[:,0].values\n",
    "\n",
    "print(X[0:3,0:3])\n",
    "print(Y[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we split the data into training and testing, using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, train_size = 0.7, test_size = 0.3, random_state = 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit our Random Forests and TPOT models to this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mod.fit(X_train,Y_train)\n",
    "Y_pred = rf_mod.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_mod.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we give TPOT larger poprulations and more time (generations)to search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_mod = TPOTClassifier(generations = 10, population_size = 100, verbosity = 2, n_jobs = 11, random_state = 79)\n",
    "\n",
    "tpot_mod.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a regression problem? Let's bring in another dataset, this time about the size of burned areas instead of just burned land. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn2 = pd.read_csv('./data/forest_fires/forestfires.csv')\n",
    "print(burn2.shape)\n",
    "print(burn2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `X` and `Y` are spatial locations, and `data` is the response variable. Let's set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mask = burn2.dtypes==object # filter categorical columns using mask and turn it into a list\n",
    "cat_cols = burn2.columns[cat_mask].tolist()\n",
    "\n",
    "labels2 = pd.DataFrame()\n",
    "\n",
    "for col in cat_cols:\n",
    "        burn2[col] = le.fit_transform(burn2[col])\n",
    "        cl = list(le.classes_)\n",
    "        labels2 = pd.concat([labels2, pd.DataFrame({col: cl})],sort=False)\n",
    "\n",
    "print(labels2)                           \n",
    "burn2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = burn2.iloc[:,:-1].values\n",
    "print(X_vars.shape)\n",
    "Y_var = burn2.iloc[:,-1].values\n",
    "print(Y_var.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_vars,Y_var, train_size=0.7, test_size=0.3, random_state = 79)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Random Forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state = 79)\n",
    "\n",
    "rf_reg.fit(X_train,Y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Y_pred = rf_reg.predict(X_test)\n",
    "\n",
    "print(math.sqrt(mean_squared_error(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TPOT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_reg = TPOTRegressor(generations=5, population_size=20, verbosity=2, random_state = 79)\n",
    "tp_reg.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = tp_reg.predict(X_test)\n",
    "\n",
    "print(math.sqrt(mean_squared_error(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_reg = TPOTRegressor(generations=10, population_size=50, verbosity=2, random_state = 79)\n",
    "tp_reg.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = tp_reg.predict(X_test)\n",
    "\n",
    "print(math.sqrt(mean_squared_error(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tpot_env)",
   "language": "python",
   "name": "tpot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
